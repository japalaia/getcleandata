# Adjust the width of the first column so the field names do not wrap.
setColumnWidth(myWrkBk, sheet = Bins_Sheet, column = 1, width = -1)
#   Save the workbook with the changes
saveWorkbook(myWrkBk)
#*      restore original working directory
#setwd(oldPath)					# not allowed in an R node
#*
#*************************************************************************
#*
#*                 End of Program IBM_Binning
#*
#*************************************************************************
if (workbookName == "") stop("Cancelled by user - No workbook selected")
#  open the workbook
myWrkBk <- loadWorkbook(workbookName, create = FALSE)
#  Read the control information on the Actions tab - returned as a data.frame
my_rdf = Get_Actions_From_Excel(myWrkBk)
if (typeof(my_rdf) != 'list')
{ 	print("Unable to get actions from workbook - check formatting")
return()
}
#
#  Read the file names and other setting from the control workbook
# returns a data.frame with (Sample_File_Name, Cluster_File_Name, Corner_Selected_Field_Name, Assign_File_Name, Assign_Out_Name, Cluster_Cnt, MAXBINS, Status)
#
my_xlf = Get_FileNames_From_Excel(myWrkBk)
if (typeof(my_xlf) != 'list')
{ 	print("Unable to get file names from workbook - check formatting")
return()
}
#*  extract some of the info for later usage
Count_of_Variables = sum(my_rdf$Clust_Flag)				# Count_of_Variables = count of clustering variables
Count_of_binvars = sum(my_rdf$Bin_Flag)
Data_Path = as.character(my_xlf$Data_Path[1])
# add a terminating \ or / so we can append the file names - Assume there is at least one of the right kind of slashes in Data_Path
if (grepl("[\\]",Data_Path)) mySlash = "\\" else mySlash = "/"
if (substr(Data_Path,nchar(Data_Path),nchar(Data_Path)) != mySlash) Data_Path = paste(Data_Path, mySlash, sep="")
Input_File_Name = paste(Data_Path , as.character(my_xlf$Sample_File_Name[1]), sep = "")
Assign_File_Name = paste(Data_Path , as.character(my_xlf$Assign_File_Name[1]), sep = "")
Cluster_File_Name = paste(Data_Path , as.character(my_xlf$Cluster_File_Name[1]), sep = "")
Max_clusters = my_xlf$Cluster_Cnt
#
#  use the field separator from the control sheet or default to tab
my_sep = ","             # declare the separator for the input file. (old default)
my_sep = "\t"			 # declare the separator for the input file.
if (length(my_xlf$Sep_Char) == 1)
{
if (my_xlf$Sep_Char =="\\t") {
my_sep = "\t"
} else {
my_sep = as.character(my_xlf$Sep_Char)
}
}
#*
#**************************************************************************
#*
#* Read in the customer record data set - csv delimited - labels on line 1 of the data set
#* Just read the labels so we can estimate the record size and check for errors.
#*
##cat( "Reading Variable Names From ", as.string(my_xlf$Sample_File_Name), " In ", my_xlf$Data_Path[1], "\n")
#*
Cluster_fields = read.csv (Cluster_File_Name, sep = my_sep, nrows = 1)
Cluster_fields = colnames(Cluster_fields)
#*nrows = 1
Report_Var_Names = as.character(my_rdf$Report_Var_Name)
UC_Report_Var_Names = toupper(my_rdf$Report_Var_Name)
#  build a list of expected clustering variable names from control wbk, add "B_" prefix for binned vars
Binned_Var_Names = Report_Var_Names
Binned_Var_Names[which(my_rdf$Bin_Flag == 1, arr.ind=TRUE)] = paste("B_",Binned_Var_Names[which(my_rdf$Bin_Flag == 1, arr.ind=TRUE)], sep="")
# Build a shorter list of clustering variables
Clust_Var_Lbl = Binned_Var_Names[which(my_rdf$Clust_Flag == 1, arr.ind=TRUE)]
#
#X=OVERLAP(Report_Var_Names, Cluster_fields A B)
if (sum(is.element(Cluster_fields, Binned_Var_Names)) != (Count_of_Variables+1)) {
print( "The following variables used for Clustering were not found in the Sample File")
Binned_Var_Names[is.element(Cluster_fields,Binned_Var_Names) == FALSE]
#return()
}
#FIXME  keep_Columns = locs(B == 1)        #$ keep positions of all the variables used.
#*
#**************************************************************************
#*
#* Read in the sample record as a .csv file
#*
#**************************************************************************
#*
Record_Count = 0
#*
cat("Reading Data From ", Cluster_File_Name, " In ","\n")
#*
#*  Read in the entire file
#   adjust for version differences in read.table command (if running older than 2 you have other problems)
if (version$major == "2") {
AA=read.table(Cluster_File_Name, header = TRUE, sep = my_sep, quote = "\"'",
dec = ".", row.names = NULL, col.names = Cluster_fields, as.is = TRUE,
na.strings = "NA", colClasses = NA,
nrows = -1, skip = 0,
check.names = TRUE, fill = FALSE,
strip.white = FALSE, blank.lines.skip = TRUE,
comment.char = "",
allowEscapes = FALSE, flush = FALSE,
stringsAsFactors = FALSE,
fileEncoding = "", encoding = "unknown", text)
} else {
AA=read.table(Cluster_File_Name, header = TRUE, sep = my_sep, quote = "\"'",
dec = ".",
row.names = NULL, col.names = Cluster_fields, as.is = TRUE,
na.strings = "NA", colClasses = NA,
nrows = -1, skip = 0,
check.names = TRUE, fill = FALSE,
strip.white = FALSE, blank.lines.skip = TRUE,
comment.char = "",
allowEscapes = FALSE, flush = FALSE,
stringsAsFactors = FALSE
#,
#fileEncoding = "", encoding = "unknown", text
)
}
Number_Of_Unique_HH = nrow(AA[1])
Record_Count = Record_Count + Number_Of_Unique_HH
#
#  strip out the Key field - Converting to a matrix rather than a dataframe speeds up the process by at least 3 orders of magnitude
#
Agency_Attribute_Set <- as.matrix(AA[Clust_Var_Lbl])
Number_Of_Mining_Attributes = ncol(Agency_Attribute_Set)			# number of columns selected for clustering
#### 	fixup bad data in clustering variables - remember to duplicate this logic in the assignment program.
###for (JK in 1:Number_Of_Mining_Attributes) {
###	errflg = 0
###	cat(colnames(A)[J])
###	LLI = which(is.na(Agency_Attribute_Set[,J]))
###	if (length(LLI) > 0) {errflg = 1; cat( " has missing", length(LLI)); Agency_Attribute_Set[LLI,J] = 0}
###	LLI = which(A[[J]] == Inf)
###	if (length(LLI) > 0) {errflg = 1; cat( " has Inf", length(LLI)); Agency_Attribute_Set[LLI,J] = 0}
###	LLI = which(A[[J]] == Inf)
###	if (length(LLI) > 0)  {errflg = 1; cat( " has -Inf", length(LLI)); Agency_Attribute_Set[LLI,J] = 0}
###	cat("\n")
###}
#
#   Set these up as global variables so the Find Corners functions can return values to them
#	Original version returned Target_Corner_Attributes, Used_List, Avg_Corner_Diff,corner_cnt
#
Avg_Corner_Diff <<- Target_Corner_Attributes <<- array(0, dim=c(Max_clusters,Number_Of_Mining_Attributes))
#
#    call the algorithm to find the center points using variable sized shapes
#
Corner_Attr_Var = Find_Var_Corners(my_xlf$Cluster_Cnt)
#
#  Save the results to the control workbook
Post_To_Excel(myWrkBk, corner_cnt, Corner_Attr_Var, Clust_Var_Lbl, Avg_Corner_Diff, 2, "Variable Sized Corners")
#**************************************************************************************************
#
#  clear out the global variables so we can detect failures more quickly
corner_cnt[] = 0
Corner_Attr_Var[] = 0
Avg_Corner_Diff[] = 0
#
#    call the algorithm to find the center points using fixed sized shapes
#
Corner_Attr_Fixed = Find_Fixed_Corners(my_xlf$Cluster_Cnt)
#
#  Save the results to the control workbook
Post_To_Excel(myWrkBk, corner_cnt, Corner_Attr_Fixed, Clust_Var_Lbl, Avg_Corner_Diff, 1, "Fixed Sized Corners")
#
#  Save the workbook with the changes
saveWorkbook(myWrkBk)
#
#*************************************************************************
#*
#*                 End of Program IBM_Clustering
#*
#*************************************************************************
if (workbookName == "") stop("Cancelled by user - No workbook selected")
#
myWrkBk <- loadWorkbook(workbookName, create = FALSE)
#**************************************************************************
#* read in the file names from the control Excel workbook
#* returns data.frame(Report_Var_Name, Rpt_Var_Type, Report_Var_Sheet, Bin_Flag, Clust_Flag, Analysis_Flag, All_Var_Label, Best_Score)
#
my_rdf = Get_Actions_From_Excel(myWrkBk)
if (typeof(my_rdf) != 'list')
{ 	print("Unable to get actions from workbook - check formatting")
return()
}
if (typeof(my_rdf) != 'list')
{ 	print("Unable to get actions from workbook - check formatting")
return()
}
#
#  Read the file names and other setting from the control workbook
#* returns data.frame(Data_Path, Sample_File_Name, Cluster_File_Name, Corner_Selected_Field_Name, Assign_File_Name, Assign_Out_Name, Cluster_Cnt, Max_bin, Status)
#
my_xlf = Get_FileNames_From_Excel(myWrkBk)
if (my_xlf$Status != 0)
{ 	print("Unable to get file names from workbook - check formatting")
return()
}
#*  extract the info for later usage
Count_of_Variables = length(my_rdf$Report_Var_Name)				# Count_of_Variables = noels(Rpt_V_Name)
Count_of_binvars = sum(my_rdf$Bin_Flag)
Data_Path = as.character(my_xlf$Data_Path[1])
# add a terminating \ or / so we can append the file names - Assume there is at least one of the right kind of slashes in Data_Path
if (grepl("[\\]",Data_Path)) mySlash = "\\" else mySlash = "/"
if (substr(Data_Path,nchar(Data_Path),nchar(Data_Path)) != mySlash) Data_Path = paste(Data_Path, mySlash, sep="")
Input_File_Name = paste(Data_Path , as.character(my_xlf$Sample_File_Name[1]), sep = "")
Assign_File_Name = paste(Data_Path , as.character(my_xlf$Assign_File_Name[1]), sep = "")
Assign_Out_Name = paste(Data_Path , as.character(my_xlf$Assign_Out_Name[1]), sep = "")
Cluster_File_Name = paste(Data_Path , as.character(my_xlf$Cluster_File_Name[1]), sep = "")
#
#  use the field separator from the control sheet or default to tab
my_sep = ","             # declare the separator for the input file. (old default)
my_sep = "\t"			 # declare the separator for the input file.
if (length(my_xlf$Sep_Char) == 1)
{
if (my_xlf$Sep_Char =="\\t") {
my_sep = "\t"
} else {
my_sep = as.character(my_xlf$Sep_Char)
}
}
#
#* remove any blank field names - Should not be any
#*
L = which(!is.na(my_rdf$Report_Var_Name), arr.ind=TRUE)
All_Names = as.character(my_rdf$Report_Var_Name[L])
All_Labels = my_rdf$All_Var_Label[L]
Score_Type = my_rdf$Best_Score[L]
All_Names_U = toupper(All_Names)       # convert pretty name to uppercase for comparison
UC_Report_Var_Names = toupper(my_rdf$Report_Var_Name)
#**************************************************************************
#* get the names of the variables in the mining record file used for clustering
#**************************************************************************
#*
Segment_var_names=array(my_rdf$Report_Var_Name[my_rdf$Clust_Flag == 1])
#**************************************************************************
#*
#* Read in the customer record data set - csv delimited - labels on line 1 of the data set
#* Just read the labels so we can estimate the record size and check for errors.
#*
Col_Names = read.csv (Cluster_File_Name, sep = my_sep, nrows = 1)
Cluster_Names = colnames(Col_Names)
Cluster_fields = Cluster_Names[2:length(Cluster_Names)]
#*
#**************************************************************************
#*
#* Read in the header of the mining record from (Assignment) .csv file
#* Check the variables using the labels from the corner attributes file
#*
#**************************************************************************
#*
Col_Names = read.csv(Assign_File_Name, sep = my_sep, nrows = 1)
Col_Names = colnames(Col_Names)							# all we really want for now is the column names
#*
#  X=OVERLAP(Segment_var_names, Col_Names, A, B)
if (sum(is.element(Cluster_fields, Col_Names)) != length(Cluster_fields)) {
print( "The following variables used for Clustering were not found in the Sample File")
print(Col_Names[is.element(Cluster_fields,Col_Names) == FALSE])
#return()
}
#* Get the Unbinned variable names
Unbinned <- All_Names
"Starting Assignment Process"
#**************************************************************************
#*
#*	Read in active set of corner attributes from excel
#*
Set_Selected = Get_Target_Corner_Selected_From_Excel(myWrkBk)
if (Set_Selected != 2) Set_Selected <- 1							# Default to 1
#
#------------------- loop through both sets of attributes ------------------------
# Only output the file for the Active attribute set. Create reports for both sets.
#
for (Cluster_Set in 1:2)
{
#
Target_Corner_Attributes    = Get_Target_Corner_Attributes_From_Excel(myWrkBk, Cluster_Set)
#*				Create arrays to hold statics about the groups
Corner_Count = array(0, dim=nrow(Target_Corner_Attributes))
Sum_Corner_Diff = array(0, dim=c(nrow(Target_Corner_Attributes),ncol(Target_Corner_Attributes)))
Number_Of_Corners = nrow(Target_Corner_Attributes)
Number_Of_Mining_Attributes = ncol(Target_Corner_Attributes)
#* setup arrays to hold the average values for both the binned and un binned variables
Avg_Corner <- array(0, dim=c(Number_Of_Corners, Number_Of_Mining_Attributes))
Unbin_avg <- array(0, c(Number_Of_Corners, length(Unbinned)))
colnames(Unbin_avg) = Unbinned
Unbin_stddev <- Unbin_avg				#copy the array of zeroes for storing standdev
#*************************************************************************
#*   File Chunking Loop
#* estimate how many records we can read and break the file into bite size chunks
have_left = memory.limit() - memory.size()
Chunk_Size = as.integer(1000000*have_left / (2*Number_Of_Mining_Attributes)+length(Col_Names))
# need to fine tune this to get a Goldielocks number
Chunk_Size = 2000000		 ##FIXME for testing
#**************************************************************************
#*
#* Read in the sample record as a .csv file
#* Name the variables using the labels in the sample record file
#*
#**************************************************************************
#*
Record_Count = 0
Rows_Read = Chunk_Size
while (isTRUE(all.equal(Rows_Read, Chunk_Size)))
{
Next_Chunk = Record_Count
cat("Reading Data From ", Assign_File_Name, " In ", getwd(), "At record ", Next_Chunk, "FOR ", Chunk_Size,"\n")
#*
#**************************************************************************
#*
#* Read in the mining record as a .csv file
#* Name the variables using the labels in the mining record file
#*
#**************************************************************************
#   adjust for version differences in read.table command (if running older than 2 you have other problems)
if (version$major == "2") {
AA=read.table(Assign_File_Name, header = TRUE, sep = my_sep, quote = "\"'",
dec = ".", row.names = NULL, col.names = Col_Names, as.is = TRUE,
na.strings = "NA", colClasses = NA,
nrows = Chunk_Size, skip = Next_Chunk,
check.names = TRUE, fill = FALSE,
strip.white = FALSE, blank.lines.skip = TRUE,
comment.char = "",
allowEscapes = FALSE, flush = FALSE,
stringsAsFactors = FALSE,
fileEncoding = "")
} else {
AA=read.table(Assign_File_Name, header = TRUE, sep = my_sep, quote = "\"'",
dec = ".",
row.names = NULL, col.names = Col_Names, as.is = TRUE,
na.strings = "NA", colClasses = NA,
nrows = Chunk_Size, skip = Next_Chunk,
check.names = TRUE, fill = FALSE,
strip.white = FALSE, blank.lines.skip = TRUE,
comment.char = "",
allowEscapes = FALSE, flush = FALSE,
stringsAsFactors = FALSE,
fileEncoding = "")
}
Rows_Read = nrow(AA[1])
Record_Count = Record_Count + Rows_Read
cat(Rows_Read, "Rows read\n\n")
#*
#* Create the mining array and compare attributes to create the hyperplane corners
#*
Agency_Attribute_Set <- as.matrix(AA[Cluster_fields])
# convert the clustering attributes
Agency_Attribute_Set <- array(as.numeric(Agency_Attribute_Set), dim=dim(Agency_Attribute_Set))
Number_Of_Mining_Attributes = ncol(Agency_Attribute_Set)			# number of columns in input array
#*
#*  Assign each agency to a corner by adding the sum of the difference between the agency's and the corner's attributes
#*  and picking the lowest total.
#*
#   Now score the entire input file
#
Corner_Selected = AssignToCorner(Target_Corner_Attributes)
#
#  	Compute the frequency of each cluster(corner) for reporting
F2 = table(Corner_Selected)
F1 = as.integer(names(F2))
Corner_Count = array(0, dim=nrow(Target_Corner_Attributes))		# create an array to hold the counts of members in each corner
Corner_Count[F1] = F2									#* put in correct place in array in case there are voids
#*
#*
#*  get the average value for the attribute of each segment (corner)
#*
for (J in 1:length(Corner_Count))
{
L = which(Corner_Selected == J, arr.ind=TRUE)
if (length(L) <= 0)
{
Avg_Corner[J,] = 0
Unbin_avg[J,] = 0
} else if (length(L) == 1)
{													# special case for 1 match Agency_Attribute_Set[L,] returns 1d array
Avg_Corner[J,] = Agency_Attribute_Set[L,]
Unbin_avg[J,] = AA[L]
} else
{
Avg_Corner[J,] = colMeans(Agency_Attribute_Set[L,], na.rm = TRUE)
#*      get the average and STDDEv for the Unbinned variables too if they are numeric
#*      only process for the first chuck to avoid a small slice at the end
#*		future fix use a weighted average to add in the rest of the chunks
if (Next_Chunk == 0)
{
for (U in 2:ncol(Unbin_avg))
{
if (is.numeric(AA[,U])) {
Unbin_avg[J,U] = mean(AA[,U][L], na.rm = TRUE)
Unbin_stddev[J,U] = sd(AA[,U][L], na.rm = TRUE)
}
}
}
}
}
#*
#*  save the ID field and corner in a csv file
#*
if (Cluster_Set == Set_Selected)
{
Corner_Selected = array(Corner_Selected)
AA = data.frame(AA, Corner_Selected)
#*
#* Write the mining record to a .csv file. Append to previous chunk if Next_Chunk > zero
#*
cat ("writing Assigned file to", Assign_Out_Name, "\n\n")
write.table(AA, file = Assign_Out_Name, append = (Next_Chunk > 0), quote = TRUE, sep = my_sep,
eol = "\n", na = "NA", dec = ".", row.names = FALSE,
col.names = TRUE, qmethod = c("escape", "double"),
fileEncoding = "")
}
#*
#*	Post the crosstab reports based on the first chunk - It is assumed to be a representative sample
#*	It would be better to use the whole dataset in a later version
#*  Only post the cross tab reports for the selected method
#*
if ((Next_Chunk == 0) & (Cluster_Set == Set_Selected))
{
Post_Crosstab(myWrkBk,my_rdf)
}
if (Next_Chunk == 0)
{										# save the initial averages
TTL_Avg_Corner = Avg_Corner
TTL_Corner_Count = Corner_Count
} else
{										# adjust the previous averages with this chunks values
for (R in 1:nrow(TTL_Avg_Corner))
{
for (C in 1:ncol(TTL_Avg_Corner))
{
TTL_Avg_Corner[R,C] = weighted.mean(c(TTL_Avg_Corner[R,C], Avg_Corner[R,C]), c(TTL_Corner_Count[R], Corner_Count[R]), na.rm = TRUE)
}
}
TTL_Corner_Count = TTL_Corner_Count + Corner_Count
}
}
#**************************************************************************
#* end of File chunking loop Loop - Continue until all records have been assigned
#**************************************************************************
#* post the cluster characteristics back to the the control sheet
if  (Cluster_Set == 1) {
CSheet = "Cluster Report Fixed"
} else {
CSheet = "Cluster Report Varying"
}
Post_Cluster_Rpt(myWrkBk, CSheet, Avg_Corner, Unbin_avg, Unbin_stddev, TTL_Corner_Count, Cluster_Set, my_rdf)
}  # end of Cluster set loop
#
#  Save the workbook with the changes
saveWorkbook(myWrkBk)
#
cat ("Assigned file for Attribute Set", Set_Selected, "written to", Assign_Out_Name, "\n")
cat ("Reports can be found in", workbookName, "\n\n")
#  End
#*************************************************************************
rm(list=ls())
setwd("C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\train\\")
# Read in the Data
train_data<-read.table("subject_train.txt")
xtrain_data<-read.table("x_train.txt")
ytrain_data<-read.table("y_train.txt")
names(xtrain_data)
head(train_data)
table(train_data)
table(xtrain_data)
head(xtrain_data)
head(ytrain_data)
table(ytrain_data)
table(train_data)
names(train_data)
summary(xtrain_data)
features<- read.table("features.txt")
features<- read.table("features.txt", sep="\t")
setwd("C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\train\\")
features<- read.table("features.txt", sep="\t")
help(read.table)
features<- read.delim("features.txt", sep="\t")
features<- read.delim(""C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\features.txt", sep="\t")
features<- read.delim("C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\features.txt", sep="\t")
head(features)
nrow(features)
features<- read.delim("C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\features.txt", sep="\t", header=FALSE)
nrow(features)
head(561)
head(features)
tail(features)
features<- read.delim("C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\features.txt", sep="\s", header=FALSE)
features<- read.delim("C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\features.txt", sep="\\s", header=FALSE)
features<- read.delim("C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\features.txt", sep=" ", header=FALSE)
head(features)
features<- read.delim("C:\\RWork\\GetCleanData\\courseproj\\getdata_projectfiles_UCI HAR Dataset\\UCI HAR Dataset\\features.txt", sep=" ", header=FALSE, strip.white=TRUE)
head(features)
head(trim.leading(features[2]))
#Functions
# returns string w/o leading whitespace
trim.leading <- function (x)  sub("^\\s+", "", x)
# returns string w/o trailing whitespace
trim.trailing <- function (x) sub("\\s+$", "", x)
# returns string w/o leading or trailing whitespace
trim <- function (x) gsub("^\\s+|\\s+$", "", x)
head(trim.leading(features[2]))
head(trim.leading(features[1]))
head(trim.leading(features[,2]))
features$V2
head(trim.leading(features[features$V2]))
head(trim.leading([features$V2]))
head(trim.leading(features$V2))
features2<-trim.leading(features$V2)
head(features)
head(features2)
features2
testmerge<- merge(train_data, xtrain_data)
help(merge)
table(train_data)
